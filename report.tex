\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{solmathsym}

\usepackage{fontspec}

\usepackage{float}

\usepackage{hyperref}

\usepackage{listings}

\makeatletter
\setlength{\@fptop}{0pt}
\makeatother

\newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage[accepted]{icml2020}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{cor}[theorem]{Corollary}

\graphicspath{{../plots/}}

\icmltitlerunning{Fundamental Theorem of Linear Inequalities and the Simplex Method}

\renewcommand{\underbrace}{\underbracket[0.25mm][0.7mm]}
\renewcommand{\overbrace}{\overbracket[0.25mm][0.7mm]}

\renewcommand{\vec}{\mathbf}

\makeatletter
\renewcommand*\env@matrix[1][*\c@MaxMatrixCols c]{%
  \hskip -\arraycolsep
  \let\@ifnextchar\new@ifnextchar
  \array{#1}}
\makeatother

\lstset{numbers=left, basicstyle=\footnotesize, numberstyle=\bfseries\footnotesize}

\begin{document}

\twocolumn[
\icmltitle{Fundamental Theorem of Linear Ineqaulities}

\begin{icmlauthorlist}
\icmlauthor{Yohan Song}{}
\end{icmlauthorlist}

\vskip 0.3in
]


\begin{abstract}
    The purpose of this paper is to explain the fundamental theorem of linear inequalities by borrowing Schrijver's approach. We first describe how it is analogous to a famous result in linear algebra. Then show how the Schrijver's approach, which we will call \textit{Schrijver's algorithm} shortly, through examples. The focus is then shifted to the simplex method, and in particular, how its variant Bland's rule is analogous to Schrijver's algorithm. 
\end{abstract}

\section{Introduction}

\subsection{Notations}

We briefly go over the notations that we are going to use throughout.
\begin{itemize}
    \item We write $\N$ to denote the \textit{set of positive integers}.
    \item We write $\F$ to denote $\Q$, the \textit{field of rational numbers}, or $\R$, the \textit{field of real numbers}. That is, any statement stated with $\F$ holds for both $\Q$ and $\R$.
    \item We write $M_{m\times n}(\F)$, where $m,n\in\N$, to denote the \textit{set of $m$-by-$n$ matrices} whose entries are elements of $\F$.
    \item Given $n,m\in\N, \vec{v}_1,\ldots,\vec{v}_n\in\F^m$, we write
        \begin{equation*}
            \cone\left\lbrace \vec{v}_1,\ldots,\vec{v}_n \right\rbrace
        \end{equation*}
        to denote the \textit{convex cone} 
        \begin{equation*}
            \left\lbrace \sum^{n}_{i=1} \lambda_i\vec{v}_i: \lambda_1,\ldots,\lambda_n\geq 0 \right\rbrace
        \end{equation*}
        \textit{generated by $\vec{v}_1,\ldots,\vec{v}_n$}.
    \item Given a subset $S\subseteq\F^n$, where $n\in\N$, we write $S^{\perp}$ to denote the \textit{orthogonal complement} of $S$.
\end{itemize} 

\section{The Theorem}

\subsection{The Theorem}

We begin by showing the fundamental theorem of linear inequalities \cite{TLIP}.
\begin{theorem}[Fundamental Theorem of Linear Ineqaulities]\label{ftli}
    Let $\vec{a}_1,\ldots,\vec{a}_n,\vec{b}\in\F^m$, where $n,m\in\N$. Then exactly one of the following holds.
    \begin{enumerate}
        \item $\vec{b}\in\cone\left\lbrace \vec{a}_i \right\rbrace ^{n}_{i=1}$.
        \item There exists $\vec{c}\in\F^m$ such that the hyperplane
            \begin{equation*}
                P = \left\lbrace \vec{x}\in\F^m: \vec{c}^{T} \vec{x} = 0 \right\rbrace 
            \end{equation*}
            has $t-1$ linearly independent vectors from $\vec{a}_1,\ldots,\vec{a}_n$ and that $\vec{c}^{T} \vec{b}<0$ and $\vec{c}^{T}\vec{a}_i\geq 0$ for all $i\in\left\lbrace 1,\ldots,n \right\rbrace$, where $t=\rank\left\lbrace \vec{a}_1,\ldots,\vec{a}_n,b \right\rbrace$.
    \end{enumerate}
\end{theorem}
The theorem itself can be quite difficult to sink in at the first glance, so before showing Schrijver's proof, we first examine similar statement in linear algebra as well as a 3-dimensional example for Theorem 1.

\subsection{Systems of Linear Equalities}

One big part of linear algebra is about solving \textit{systems of linear equalities}, which can be written as
\begin{equation}
    A\vec{x} = \vec{b}
\end{equation}
for some $A\in M_{m\times n}(\F), x\in\F^n, \vec{b}\in\F^m$, where $m,n\in\N$. If (1) has a solution, then what (1) is essentially saying is that $\vec{b}$ is a \textit{linear combination of the columns of $A$}. In other words, if we write
\begin{equation*}
    A = \begin{bmatrix} \vec{a}_1\cdots\vec{a}_n \end{bmatrix} , \vec{x} = \begin{bmatrix} \lambda_1\\\vdots\\\lambda_n \end{bmatrix} ,
\end{equation*}
then
\begin{equation}
    \vec{b} \overset{\text{(1)}}{=} A\vec{x} = \begin{bmatrix} \vec{a}_1\cdots\vec{a}_n \end{bmatrix} \begin{bmatrix} \lambda_1\\\vdots\\\lambda_n \end{bmatrix} = \sum^{n}_{i=1} \lambda_i\vec{a}_i.
\end{equation}
Conversely, if (1) does not have a solution, then $\vec{b}$ is not a linear combination of $\vec{a}_1,\ldots,\vec{a}_n$, as (2) suggests. This means $\vec{b}$ is linearly independent of $\vec{a}_1,\ldots,\vec{a}_n$. We thus obtain the following characterization of \textit{consistent} (i.e. having a solution) systems of linear equalities.

\begin{theorem}
    Let $A\in M_{m\times n}(\F), \vec{b}\in\F^m$. Then exactly one of the following holds.
    \begin{enumerate}
        \item There exists $\vec{x}\in\F^n$ such that $A\vec{x}=\vec{b}$.
        \item $\rank\left( A \right) + 1 = \rank\left[ A|\vec{b} \right]$.
    \end{enumerate}
\end{theorem}

Observe that 1 of Theorem 2 is the condition which $A\vec{x}=\vec{b}$ is consistent, and it can be phrased as follows: $\vec{b}\in\spn\left\lbrace \vec{a}_i \right\rbrace ^{m}_{i=1}$. This is where the similarities of the two theorems are revealed: Theorem 1 is concerned about systems of linear inequalities of the form
\begin{equation}
    \begin{cases} 
        A\vec{x} & = \vec{b} \\
        \vec{x} & \geq \vec{0}
    \end{cases}.
\end{equation}
Saying that (3) is \textit{feasible} is equivalent to 1 of Theorem 1, similar to how the consistency of $A\vec{x}=\vec{b}$ is equivalent to $\vec{b}\in\spn\left\lbrace \vec{a}_i \right\rbrace ^{m}_{i=1}$. We also note that the competing conditions -- 2's of Theorem 1, 2 -- are also similar, but this similarity is different from how 1's of Theorem 1, 2 are similar to each other.

Say a student is asked to solve a system $A\vec{x}=\vec{b}$ and report if it does not have a solution. Suppose, after going through Gauss-Jordan elimination on $\left[ A|\vec{b} \right]$, the student finds that $A\vec{x}=\vec{b}$ is inconsistent. Then, the student have two options to prove the conclusion:
\begin{enumerate}
    \item show the whole elimination process, which could be quite long; or
    \item use Theorem 2: calculate $\rank\left[ A|\vec{b} \right]$ and show that it is 1 greater than $\rank\left( A \right)$.
\end{enumerate}
Arguably the latter approach is simpler for checking if the result is correct or not. That is, 2 of Theorem 2 is useful when we desire to provide a \textit{certificate} for inconsistency of a system of linear equalities.

In a similar manner, 2 of Theorem 1 provides \textit{what} can be a certificate, namely $\vec{c}$, for the case which a system of linear inequalities of the form (3) is infeasible, although it does not say \textit{how} we can obtain one. We demonstrate the \textit{how} part with an example in the following subsection.

\subsection{A Three-dimensional Example of The Theorem}

Say we have the following:
\begin{equation}
    A =
    \begin{bmatrix}
    	1 & 1 & -1 & -1 \\
    	1 & -1 & 1 & -1 \\
    	3 & 3 & 3 & 3 \\
    \end{bmatrix},
    \vec{b} = \begin{bmatrix} 2 \\ 2\\ 0 \end{bmatrix}.
\end{equation}
Figure 1 shows the column vectors
\begin{equation*}
    \vec{a}_1 = \begin{bmatrix} 1\\1\\3 \end{bmatrix} , \vec{a}_2 = \begin{bmatrix} 1\\-1\\3 \end{bmatrix} , \vec{a}_3 = \begin{bmatrix} -1\\1\\3 \end{bmatrix}, \vec{a}_4 = \begin{bmatrix} -1\\-1\\3 \end{bmatrix} 
\end{equation*}
of $A$ and $\vec{b}$.

\begin{figure}
    \center
    \caption{Column vectors $\vec{a}_1,\ldots,\vec{a}_4$ of $A$ and $\vec{b}$}
    \includegraphics[width=0.5\textwidth]{ex1.png}
\end{figure}

From Figure 1, it \textit{seems clear} that $\vec{b}\notin\cone\left\lbrace \vec{a}_{i} \right\rbrace^{4}_{i=1}$. To concretely see this, we demonstrate \textit{Schrijver's Algorithm}, an algorithm provided by Schrijver\cite{TLIP} in his proof of Theorem 1. Note that the following algorithm assumes that $\spn\left\lbrace \vec{a}_i \right\rbrace ^{n}_{i=1} = \F^m$; cases which $\vec{a}_1,\ldots,\vec{a}_n$ do not span the whole space will be dealt shortly.

\begin{algorithm}[]
   \caption{Schrijver's Algorithm}
   \label{alg:Schrijver}
\begin{algorithmic}
    \STATE {\bfseries Input:} Column vectors $\vec{a}_1,\ldots,\vec{a}_n,\vec{b}\in\F^m$, where $n,m\in\N$ and $\spn\left\lbrace \vec{a}_i \right\rbrace ^{n}_{i=1}=\F^m$.
    \STATE Choose linearly independent $\vec{a}_{i_1},\ldots,\vec{a}_{i_m}\in\left\lbrace \vec{a}_i \right\rbrace ^{n}_{i=1}$.
    \REPEAT
    \STATE Write $\vec{b}$ as a linear combination $\vec{b} = \sum^{m}_{j=1} \lambda_{i_j}\vec{a}_{i_j}$ of $\vec{a}_{i_1},\ldots,\vec{a}_{i_m}$, where $\lambda_{i_1},\ldots,\lambda_{i_j}\in\F$.
    \IF{$\forall j\in\left\lbrace 1,\ldots,m \right\rbrace \left[ \lambda_{i_j}\geq 0 \right] $}
    \STATE We are at case 1 of Theorem 1.
    \STATE Terminate the loop.
    \ENDIF
    \STATE Let $h = \min\left\lbrace i\in\left\lbrace i_1,\ldots,i_m \right\rbrace : \lambda_i<0 \right\rbrace$.
    \STATE Consider $P = \spn\left( \left\lbrace \vec{a}_{i_j} \right\rbrace^{n}_{j=1, i_j\neq h} \right)$.
    \STATE Choose nonzero $\vec{c}\in P^{\perp}$ with $\vec{c}^{T} \vec{a}_h > 0$.
    \IF{$\forall j\in\left\lbrace 1,\ldots,m \right\rbrace \left[ \vec{c}^{T} \vec{a}_{i_j}\geq 0 \right]$}
    \STATE We are at case 2 of Theorem 1.
    \STATE Terminate the loop and report $\vec{c}$.
    \ENDIF
    \STATE Choose $s = \min\left\lbrace i\in\left\lbrace 1,\ldots,n \right\rbrace : \vec{c}^{T} \vec{a}_i < 0 \right\rbrace$.
    \STATE Replace $\vec{a}_h$ with $\vec{a}_s$.
    \UNTIL{}
\end{algorithmic}
\end{algorithm}
Let us see how Schrijver's algorithm works on our example. We first choose $\vec{a}_1,\vec{a}_2,\vec{a}_3$, which forms a basis, say $\beta$, for $\F^n$. Observe that solving
\begin{equation*}
    \begin{bmatrix} \vec{a}_1&\vec{a}_2&\vec{a}_3 \end{bmatrix} \vec{x} = \vec{b}
\end{equation*}
gives $\vec{x} = \left( 2,-1,-1 \right)$, so
\begin{equation*}
    \vec{b} = 2\vec{a}_1-\vec{a}_2-\vec{a}_3 = \underbrace{\left( 2 \right) }_{=\lambda_1}\vec{a}_1+\underbrace{\left( -1 \right) }_{=\lambda_2}\vec{a}_2+\underbrace{\left( -1 \right) }_{=\lambda_3}\vec{a}_3
\end{equation*}
is the unique way of writing $\vec{b}$ as a linear combination of $\vec{a}_1,\vec{a}_2,\vec{a}_3$. Moreover, we see that $\lambda_2,\lambda_3<0$, so the body of the first if in the loop is not being executed, and we proceed to find $h$, which is 2 for this case. This means
\begin{equation*}
    P = \spn\left\lbrace \vec{a}_1, \vec{a}_3 \right\rbrace,
\end{equation*}
and observe that
\begin{equation*}
    \vec{v} = \begin{bmatrix} 0\\3\\-1 \end{bmatrix} 
\end{equation*}
is orthogonal to both $\vec{a}_1,\vec{a}_3$, which means $\vec{v}\in P^{\perp}$. But note that
\begin{equation*}
    \vec{v}^{T} \vec{a}_2 = \begin{bmatrix} 0&3&-1 \end{bmatrix} \begin{bmatrix} 1\\-1\\3 \end{bmatrix} = -6 < 0,
\end{equation*}
so we scale $\vec{v}$ by $\left( -1 \right)$: let $\vec{c}=-\vec{v}$. Then we know that
\begin{equation*}
    \vec{c}^{T} \vec{a}_1 = \vec{c}^{T} \vec{a}_3 = 0, \vec{c}^{T} \vec{a}_2\geq 0
\end{equation*}
from above, and also note that
\begin{equation*}
    \vec{c}^{T} \vec{a}_4 = \begin{bmatrix} 0&-3&1 \end{bmatrix} \begin{bmatrix} -1\\-1\\3 \end{bmatrix} = 6 \geq 0.
\end{equation*}
So indeed Schrijver's algorithm successfully produces a certificate $\vec{c}$ for $\vec{b}\notin\cone\left\lbrace \vec{a}_{i} \right\rbrace^{4}_{i=1} $.

In fact, this example is simple enough that we can directly verify $\vec{b}\notin\cone\left\lbrace \vec{a}_{i} \right\rbrace^{4}_{i=1} $: suppose that
\begin{equation*}
    \vec{b} = \sum^{4}_{i=1} \lambda_i\vec{a}_i
\end{equation*}
for some nonnegative $\lambda_1,\ldots,\lambda_4$ for the sake of contradiction. If $\lambda_1=\cdots=\lambda_4=0$, then $\sum^{4}_{i=1} \lambda_i\vec{a}_i =\vec{0}$, so we face a contradiction. On the other hand, if some $\lambda_i\neq 0$, then the last component of $\sum^{4}_{i=1} \lambda_i\vec{a}_i$ is nonzero, so again, a contradiction occurs.

\subsection{Correctness of Schrijver's Algorithm}

It is clear from Schrijver's algorithm that if the loop terminates, then Schrijver's algorithm is correctly reports at which case we are. So let us prove this fact.
\begin{lemma}
    Given $\vec{a}_1,\ldots,\vec{a}_n,\vec{b}\in\F^m$, where $n,m\in\N$, such that $\spn\left\lbrace \vec{a}_i \right\rbrace ^n_{i=1}=\F^m$, Schrijver's algorithm terminates.
\end{lemma}

\begin{proof}
    \cite{TLIP} Suppose, for the sake of contradiction, that the loop does not terminate. For all $r\in\N$, let $\beta_r$ to be the set of $m$ linearly independent $\vec{a}_i$'s used during the $r$th iteration of the loop. Since we have only finitely many choices for $\beta_r$, there exist $l,k\in\N$ such that $\beta_l=\beta_k$. We may assume that $l<k$.

    This means we have $k-l$ $h$'s, say $h_l,\ldots,h_{k-1}\in\left\lbrace 1,\ldots,n \right\rbrace$, that are the indices of the $\vec{a}_i$'s \textit{popped off} from the corresponding $\beta_r$. Let
    \begin{equation*}
        H = \max\left\lbrace h_r \right\rbrace^{k-1}_{r=l}
    \end{equation*}
    and let $p\in\left\lbrace l,\ldots,k-1 \right\rbrace$ be such that $\beta_p\setminus\beta_{p+1}=\left\lbrace \vec{a}_H \right\rbrace$ (i.e. $\vec{a}_H$ is \textit{popped off} from the basis $\beta_p$ for $\F^m$ that we have at $p$th iteration). Since $\beta_k=\beta_l$, there exists some other index $q\in\left\lbrace l,\ldots,k-1 \right\rbrace$ such that $\vec{a}_H$ is \textit{pushed in} to the basis at the $q$ th iteration (i.e. $\beta_{q+1}\setminus\beta_q=\left\lbrace \vec{a}_H \right\rbrace$). Let $\vec{c}_q\in\F^m$ be the vector $\vec{c}$ that we obtain at $q$th iteration, that
    \begin{equation*}
        \vec{c}_q^{T} \vec{a}_i = 0
    \end{equation*}
    for all $\vec{a}_i\in\beta_q, \vec{a}_i\neq \vec{a}_{h_q}$ and that
    \begin{equation*}
        \vec{c}_q^{T} \vec{a}_{h_q} > 0.
    \end{equation*}
    This means, in particular, that
    \begin{equation}
        \vec{c}_q^{T} \vec{b} = \underbrace{\lambda_{h_q}}_{<0}\underbrace{\vec{c}_q^{T} \vec{a}_{h_q}}_{>0} < 0,
    \end{equation}
    since $h_q$ is the smallest index at $q$th iteration such that the corresponding coefficient $\lambda_{h_q}$ is negative.

    On the other hand, if we write
    \begin{equation*}
        \beta_p = \left\lbrace \vec{a}_{i_j} \right\rbrace ^n_{j=1}
    \end{equation*}
    and
    \begin{equation*}
        \vec{b} = \sum^{n}_{j=1} \lambda_{i_j}\vec{a}_{i_j} 
    \end{equation*}
    for convenience, then observe that we can split $\vec{c}_q^{T} \vec{b}$ into three parts:
    \begin{equation}
        \vec{c}_q^{T}\vec{b}  = \lambda_H\vec{c}_q^{T} \vec{a}_H + \sum^{}_{\substack{1\leq j\leq n\\i_j<H}}\lambda_{i_j}\vec{c}_q^{T} \vec{a}_{i_j} + \sum^{}_{\substack{1\leq j\leq n\\i_j>H}} \lambda_{i_j}\vec{c}_q^{T} \vec{a}_{i_j}. 
    \end{equation}
    Now observe the following.
    \begin{enumerate}
        \item $\vec{a}_H$ plays the role of $\vec{a}_s$ at $q$th iteration (i.e. replacing $\vec{a}_{h_q}$), so it follows that 
            \begin{equation}
                \forall j\in\left\lbrace 1,\ldots,n \right\rbrace \left[ i_j < H \implies \vec{c}_q^{T} \vec{a}_{i_j}\geq 0 \right] ,
            \end{equation}
            and that
            \begin{equation}
                \vec{c}_q^{T} \vec{a}_H < 0.
            \end{equation}
        \item Since $H = \max\left\lbrace h_r \right\rbrace ^{k-1}_{r=l}$, it follows that, for every $i>H$, $\vec{a}_i$ is neither added to nor removed from the bases $\beta_l,\ldots,\beta_{k-1}$. So in particular,
            \begin{equation*}
                \forall j\in\left\lbrace 1,\ldots,n \right\rbrace \left[ i_j > H \implies \vec{a}_{i_j}\in \beta_q \right] ,
            \end{equation*}
            which means
            \begin{equation}
                \forall j\in\left\lbrace 1,\ldots,n \right\rbrace \left[ i_j > H \implies \vec{c}_q^{T} \vec{a}_{i_j} = 0 \right] ,
            \end{equation}
            since $i_j>H\geq h_q$ so $i_j\neq h_q$.
        \item On the other hand, at $p$th iteration, $\vec{a}_H$ is \textit{popped off}, so
            \begin{equation}
                \forall j\in\left\lbrace 1,\ldots,n \right\rbrace \left[ i_j < H \implies \lambda_{i_j}\geq 0 \right] ,
            \end{equation}
            and that
            \begin{equation}
                \lambda_H < 0.
            \end{equation}
    \end{enumerate}
    Combining (6), ..., (11) gives
    \begin{equation}
        \vec{c}_q^{T} \vec{b} > 0
    \end{equation}
    after some cleanup. But now note that (5) and (12) are contradicting each other, as desired.
\end{proof}

\begin{cor}
    Fundamental theorem of linear inequalities is true when $\vec{a}_1,\ldots,\vec{a}_n$ span the whole space.
\end{cor}	

\begin{proof}
    To see that 1, 2 of Theorem 1 exclude each other, suppose, for the sake of contradiction, that both hold. Let $\lambda_1,\ldots,\lambda_n\geq 0$ be such that
    \begin{equation*}
        \vec{b} = \sum^{n}_{i=1} \lambda_i\vec{a}_i
    \end{equation*}
    and let $\vec{c}$ be the vector mentioned in 2. Then we have
    \begin{equation*}
        0 \overset{2}{>} \vec{c}^{T} \vec{b} = \vec{c}^{T} \sum^{n}_{i=1} \lambda_i\vec{a}_i \overset{2}{\geq} 0,
    \end{equation*}
    so we have a contradiction. To see why at least one of 1, 2 occurs when the premise of the theorem is met, see the proof of Schrijver's algorithm.
\end{proof}

After his proof, Schrijver mentions that \cite{TLIP} \textit{the above proof of this fundamental theorem also gives a fundamental algorithm: it is a disguised form of the famous simplex method, with Bland's rule incorporated}.

Let us see what this means shortly. But before this, we briefly consider the case which $\vec{a}_1,\ldots,\vec{a}_n$ do not span the whole space. Suppose that
\begin{equation*}
    \underbrace{\spn\left\lbrace \vec{a}_i \right\rbrace ^n_{i=1}}_{=W}\neq\F^m.
\end{equation*}
If $\vec{b}\notin W$, then $\vec{b}\notin\cone\left\lbrace \vec{a}_{i} \right\rbrace^{n}_{i=1}\subseteq W$. Since $\dim\left( W \right) < m$, $W^{\perp}$ is nonempty so by choosing any $\vec{c}\in W^{\perp}$ with $\vec{c}^{T} \vec{b}<0$,\footnote{Concretely speaking, we choose any $\vec{v}\in W^\perp$ and if $\vec{v}^{T} \vec{b}<0$, we take $\vec{c}=\vec{v}$; otherwise, we take $\vec{c}=-\vec{v}$} we are done. Otherwise, note that we can restrict Schrijver's algorithm to $W$ and the proof of termination is essentially the same.

\section{Schrijver's Algorithm and the Simplex Method}

\subsection{Turning Inequalities to Equalities with Nonnegativity Constraints}

Suppose that we have a system of linear inequalities
\begin{equation}
    \begin{aligned}
        a_{11}z_1 + \cdots + a_{1k}z_k & \leq b_1 \\
                                           & \mathrel{\makebox[\widthof{=}]{\vdots}} \\
        a_{m1}z_1 + \cdots + a_{mk}z_k & \leq b_m
    \end{aligned} 
\end{equation}
with an objective function
\begin{equation*}
    \sum^{k}_{j=1} c_jz_j
\end{equation*}
which we desire to minimize, where $a_{ij}, b_i, c_j\in\F$. To turn (13) to a system of linear \textit{equalities} with positive constraints (i.e. a system of the form (3)), we have two tricks. 
\begin{enumerate}
    \item \textit{slack variables}: Define
        \begin{equation}
            s_i = b_i - \sum^{k}_{j=1} a_{ij}z_j
        \end{equation}
        for all $i\in\left\lbrace 1,\ldots,m \right\rbrace$. $s_i$'s are called \textit{slack variables}.
    \item \textit{difference of two positive numbers}: Since any real number can be written as a difference of two nonnegative real numbers, define $z^+_j, z^-_j$ such that
        \begin{equation}
            z_j = z^+_j - z^-_j
        \end{equation}
        for all $j\in\left\lbrace 1,\ldots,k \right\rbrace$.
\end{enumerate}
Observe that $s_i$'s and $z_j^+, z_j^-$'s are nonnegative by definition, so (13) becomes
\begin{flalign*}
    && a_{11}\left( z_1^+-z_1^- \right)  + \cdots + a_{1k}\left( z_k^+-z_k^- \right) + s_1 & = b_1 && \\
    && & \mathrel{\makebox[\widthof{=}]{\vdots}} && \\
    && a_{m1}\left( z_1^+-z_1^- \right) + \cdots + a_{mk}\left( z_k^+-z_k^- \right) + s_m & = b_m  &&
\end{flalign*} 
with a nonnegativity constraint
\begin{equation*}
    s_i, z_j^+,z_j^-\geq 0
\end{equation*}
for all $i\in\left\lbrace 1,\ldots,m \right\rbrace,j\in\left\lbrace 1,\ldots,k \right\rbrace $. Also the objective function becomes
\begin{equation*}
    \sum^{k}_{j=1} c_j\left( z_j^+-z_j^- \right) .
\end{equation*}
More compactly, we may define
\begin{equation*}
    A =
    \begin{bmatrix} 
        a_{11} & a_{11} & \cdots & a_{1k} & a_{1k} & 1 & 0 & \cdots & 0 \\
        a_{21} & a_{21} & \cdots & a_{2k} & a_{2k} & 0 & 1 & \cdots & 0 \\
        \vdots & \vdots & & \vdots & \vdots & \vdots & \vdots & \ddots & \vdots \\
        a_{m1} & a_{m1} & \cdots & a_{mk} & a_{mk} & 0 & 0 & \cdots & 1 \\
    \end{bmatrix} 
\end{equation*}
and
\begin{equation*}
    \vec{x} = \begin{bmatrix} z_1^+ \\ z_1^- \\ \vdots \\ z_k^+ \\ z_k^- \\ s_1 \\ \vdots \\ s_m \end{bmatrix} , \vec{c}=\begin{bmatrix} c_1\\c_1\\\vdots\\c_k\\c_k\\0\\\vdots\\0 \end{bmatrix} , \vec{b} = \begin{bmatrix} b_1\\\vdots\\b_m \end{bmatrix} 
\end{equation*}
to write a system equivalent to (13)
\begin{equation}
    A\vec{x}=\vec{b}, \vec{x}\geq \vec{0}
\end{equation}
with the objective function
\begin{equation*}
    \vec{c}^{T} \vec{x}.
\end{equation*}
Observe that any solution of (16) is associated with a solution of (13) by means of (14) and (15) and vice versa, which means if we minimize $\vec{c}^{T} \vec{x}$ with constraints (16), then we obtain an optimal solution for (13) as well.

For reference, denote $n = 2k+m$ so that $\vec{x},\vec{c}\in\F^n$ and $A\in M_{m\times n}(\F)$. 

\subsection{Phase II}

Suppose that we are given a polytope $P$ in some (rational or Euclidean) space, which represents the \textit{feasible region} of an LP problem. A key insight is that at least one of the vertices of $P$ is an optimal solution for the LP problem. Given this, the simplex algorithm operates as follows \cite{LPE}:
\begin{itemize}
    \item \textit{phase I}: Find a vertex $\vec{v}$ of $P$.
    \item \textit{phase II}: While we have a neighboring vertex $\vec{u}$ of $\vec{v}$ has an objective value at least that of $\vec{v}$, we move to $\vec{u}$ (i.e. update $\vec{v}$ with $\vec{u}$).
\end{itemize} 
Of course, the above description is purely geometrical, and the algebraic description is as follows.

Observe that, if we set $x_1=\cdots=x_{2k} = 0$ (i.e. the first $2k$ entries of $\vec{x}$) and $x_{2k+i} = b_i$ for all $i\in\left\lbrace 1,\ldots,m \right\rbrace $ (i.e. the last $m$ entries of $\vec{x}$), then (16) tells us that $\vec{x}$ at least satisfies the first constraint: $A\vec{x}=\vec{b}$. If $\vec{x}$ also satisfies the nonnegativity constraint $\vec{x}\geq\vec{0}$, then we have found a basic feasible solution for (16). In general, given any solution $\vec{x}$, if there are $m$ indices $j_1,\ldots,j_m\in\left\lbrace 1,\ldots,n \right\rbrace$ such that
\begin{enumerate}
    \item $x_j=0$ for all $j\in\left\lbrace 1,\ldots,n \right\rbrace \setminus \left\lbrace j_1,\ldots,j_m \right\rbrace$; and
    \item $A_{\left\lbrace j_1,\ldots,j_m \right\rbrace }$ (i.e. the square submatrix of $A$ whose columns are $j_1,\ldots,j_m$th columns of $A$, respectively) is invertible;
\end{enumerate}
then $\vec{x}$ is called a \textit{basic feasible solution}. If we remove the nonnegativity constraint, then $\vec{x}$ is called a \textit{basic solution}.

We accpet the following result without proof \cite{UULP}.

\begin{lemma}
    Let $P$ be the feasible region of an LP problem in equational form (i.e. a system of linear equalities with nonnegativity constraints). Then the following are equivalent about $\vec{v}\in P$.
    \begin{enumerate}
        \item $\vec{v}$ is a vertex.
        \item $\vec{v}$ is a basic feasible solution.
    \end{enumerate}
\end{lemma}

So, by Lemma 5, the two phases of the simplex algorithm can be equivalently written as:
\begin{itemize}
    \item \textit{phase I}: Find a basic feasible solution $\vec{v}$ of the LP problem.
    \item \textit{phase II}: While we have a neighboring basic feasible solution $\vec{u}$ of $\vec{v}$ has an objective value at least that of $\vec{v}$, we move to $\vec{u}$ (i.e. update $\vec{v}$ with $\vec{u}$).
\end{itemize} 

Throughout the rest of this subsection, let us fix an LP problem
\begin{equation}
    \begin{aligned}
        \text{minimize } & \vec{c}^{T}\vec{x} && \\
        \text{subject to } & A\vec{x}=\vec{b}&&\\
                           & \vec{x}\geq \vec{0}
    \end{aligned},
\end{equation}
where $A\in M_{m\times n}(\F), \vec{c}\in\F^n, \vec{b}\in\F^m$. We also denote
\begin{equation*}
    P = \left\lbrace \vec{x}\in\F^n: A\vec{x}=\vec{b},\vec{x}\geq\vec{0} \right\rbrace ,
\end{equation*}
the feasible region for (17). Since we are at phase II, we are also assuming that we already obtained a basic feasible solution $\vec{v}\in P$ from phase I. Let 
\begin{equation*}
    \mB^{\left( 0 \right) } = \left\lbrace j_1^{\left( 0 \right) },\ldots,j_m^{\left( 0 \right) } \right\rbrace \subseteq \left\lbrace 1,\ldots,n \right\rbrace
\end{equation*}
be the set of indices such that
\begin{equation*}
    v_j = 0
\end{equation*}
for all $j\in\left\lbrace 1,\ldots,n \right\rbrace \setminus \mB$ and that
\begin{equation}
    A_{\mB^{\left( 0 \right) }} = 
    \begin{bmatrix} \vec{a}_{j_1^{\left( 0 \right) }}&\cdots&\vec{a}_{j_m^{\left( 0 \right) }} \end{bmatrix} 
\end{equation}
is invertible, where each $\vec{a}_j$ represents the $j$th column of $A$. We may also order the set of indices, say $\mN^{\left( 0 \right) }$, that are not included in $\mB^{\left( 0 \right) }$ in some way and define $A_{\mN^{\left( 0 \right) }}$ analogous to (18). Then, the matrix
\begin{equation*}
    \begin{bmatrix} A_{\mB^{\left( 0 \right) }} & A_{\mN^{\left( 0 \right) }}\end{bmatrix} 
\end{equation*}
is obtained by \textit{swapping rows} of $A$. Since we assumed $A_{\mB^{\left( 0 \right) }}$ is invertible, we may perform Gauss-Jordan elimination on it:
\begin{equation*}
    A_{\mB^{\left( 0 \right) }}^{-1} \begin{bmatrix} A_{\mB^{\left( 0 \right) }} & A_{\mN^{\left( 0 \right) }}|\vec{b}\end{bmatrix} = \begin{bmatrix} I & A_{\mB^{\left( 0 \right) }}^{-1} A_{\mN^{\left( 0 \right) }} |A_{\mB^{\left( 0 \right)}}^{-1} \vec{b}\end{bmatrix} .
\end{equation*}
Note that we are utilizing the fact that swapping rows does not affect the overall sum, as long as we make the same changes for the order which the entries of the vector that is right-multiplied occur. That is, if we define $\vec{v}_{\mB^{\left( 0 \right) }}, \vec{v}_{\mN^{\left( 0 \right) }}$ analogous to (18), then
\begin{equation*}
    \begin{bmatrix} I & A_{\mB^{\left( 0 \right) }}^{-1} A_{\mN^{\left( 0 \right) }} \end{bmatrix} \begin{bmatrix} \vec{v}_{\mB^{\left( 0 \right) }}\\\vec{v}_{\mN^{\left( 0 \right) }} \end{bmatrix} = A_{\mB^{\left( 0 \right) }}^{-1} \vec{b},
\end{equation*}
meaning that
\begin{equation*}
    \vec{v}_{\mB^{\left( 0 \right) }} = A_{\mB^{\left( 0 \right) }}^{-1} \vec{b},
\end{equation*}
since we know $\vec{v}_{\mN^{\left( 0 \right) }}=\vec{0}$.

To make our discussion little easier, let us think this process in terms of \textit{variables}: that is, we are going to use $\vec{x}=\left( x_1,\ldots,x_n \right)$ as placeholder for a feasible solution for (17) (or, equivalently, a point of $P$), and we are going to treat the entries $x_1,\ldots,x_n$ to be variables. Then the \textit{basic variables} associated with $\mB^{\left( 0 \right) }$ are $x_{j_1},\ldots,x_{j_m}$, and other variables are called \textit{nonbasic} (this shows why we denoted the sets with $\mB,\mN$, respectively). 

After the elimination, there is a convenient test for an optimality. First, denote
\begin{equation*}
    z = \vec{c}^{T} \vec{x} = \begin{bmatrix} \vec{c}_{\mB^{\left( 0 \right) }}^{T} & \vec{c}_{\mN^{\left( 0 \right) }}^{T} \end{bmatrix} \begin{bmatrix} \vec{x}_{\mB^{\left( 0 \right) }}\\\vec{x}_{\mN^{\left( 0 \right) }} \end{bmatrix} ,
\end{equation*}
the objective function. If we use the augmented matrix notation, then we would instead write:
\begin{equation*}
    \begin{bmatrix} \vec{c}_{\mB^{\left( 0 \right) }}^{T} & \vec{c}_{\mN^{\left( 0 \right) }}^{T} | z \end{bmatrix} .
\end{equation*}
This viewpoint allows us to combine the (eliminated) system of linear equalities and the objective function into a single augmented matrix:
\begin{equation*}
    \begin{bmatrix}[cc|c]
        I & A_{\mB^{\left( 0 \right) }}^{-1} A_{\mN^{\left( 0 \right) }} & A_{\mB^{\left( 0 \right)}}^{-1} \vec{b} \\
        \vec{c}_{\mB^{\left( 0 \right) }}^{T} & \vec{c}_{\mN^{\left( 0 \right) }}^{T} & z \\
    \end{bmatrix} .
\end{equation*}
By utilizing the fact that we have an identity block, we can \textit{eliminate} basic variables from the objective function:
\begin{equation}
    \begin{bmatrix}[cc|c]
        I & A_{\mB^{\left( 0 \right) }}^{-1} A_{\mN^{\left( 0 \right) }} & A_{\mB^{\left( 0 \right)}}^{-1} \vec{b} \\
        \vec{0}^{T} & \vec{c}'^{T}  & z -d \\
    \end{bmatrix} ,
\end{equation}
where
\begin{equation*}
    \vec{c}' = \vec{c}_{\mN^{\left( 0 \right) }}-\vec{c}^{T} _{\mB^{\left( 0 \right) }}A^{-1} _{\mB^{\left( 0 \right) }}A_{\mN^{\left( 0 \right) }}
\end{equation*}
and
\begin{equation*}
    d = \vec{c}_{\mB^{\left( 0 \right) }}^{T} A^{-1} _{\mB^{\left( 0 \right) }}\vec{b}.
\end{equation*}
This means the objective function can be described entirely by using nonbasic variables only, namely
\begin{equation*}
    z=\vec{c}'^{T} \vec{x}_{\mN^{\left( 0 \right) }}+d.
\end{equation*}
When we substitute $\vec{v}_{\mN^{\left( 0 \right) }}$ in place of $\vec{x}_{\mN^{\left( 0 \right) }}$, 
\begin{equation*}
    z = \vec{c}'^{T} \vec{v}_{\mN^{\left( 0 \right) }}+d.
\end{equation*}
Now the interesting part is $\vec{c}' = \left( c_i \right)_{i\in\mN^{\left( 0 \right) }} $ but not the constant term afterwards. The optimality test then utilizes the fact that, when we change any variable, we have to change at least one nonbasic variable, and that we may change a nonbasic variable only in a way that it increases.
\begin{enumerate}
    \item If each $c_i'>0$, then changing (i.e. increasing) any nonbasic variables results in the increase in $z$. Since $z$ is completely described as a linear combination of nonbasic variables (up to a constant), it follows that $\vec{v}$ is optimal.
    \item If there is some index $i\in\mN^{\left( 0 \right) }$ such that $c_i'=0$, then increasing $x_i$ does not affect the value of $z$. In this case, the test is inconclusive.
    \item If there is some index $i\in\mN^{\left( 0 \right) }$ such that $c_i'<0$, then increasing $x_i$ reduces the value of $z$. Hence $\vec{v}$ is not optimal.
\end{enumerate}
If 1 occurs, then we can simply report $\vec{v}$; otherwise, in both cases 2, 3 (say $i=s$ is such that $c_i'\leq 0$), we try to increase $x_s$ \textit{as much as possible}.\footnote{Note that there could be more than one choices for $s$ here; we will come back to this shortly when we discuss Bland's rule.} That is, each row (except the last row) of (19) describes how much $x_s$ can be increased: suppose that the $h$th row of (19) is
\begin{equation*}
    \begin{bmatrix}[cccccccccc|c] 0 & \cdots & 0 & 1 & 0 & \cdots & 0 & a'_{h\left( m+1 \right) } & \cdots & a'_{hn} & b'_h\end{bmatrix} .
\end{equation*}
Then by fixing other nonbasic variables as constants and assuming that $a_{hs}'\neq 0$, the largest magnitude for $x_s$ is $\left|\frac{b_h'}{a_{hs}'}\right|$ (i.e. by setting $x_{j_h}=0, x_s=\frac{b_h'}{a_{hs}'}$, the equality for the $h$th row still holds). So we consider the ratios $\frac{b_1'}{a_{1s}},\ldots,\frac{b_m'}{a_{ms}}$. Since we want $x_s$ to be nonnegative at the end of the day, we better exclude $\frac{b_h'}{a_{hs}'}$'s that are negative. Moreover, among the nonnegative $\frac{b_h'}{a_{hs}'}$'s, we have to choose the smallest one, since otherwise the nonnegativity constraint for every $x_{j_k}$'s with $\frac{b_k'}{a_{ki}'}<\frac{b_h'}{a_{hs}'}$ is broken.\footnote{A subtlety occurs when there is a tie; we will come back to this shortly when we discuss Bland's rule.}

After we choose the $h$th row, by using elementary operations, we eliminate $x_s$ from every other row, including the row that describes the objective function. Then by swapping $h$th column and $i$th column (which can be thought as interchanging the $h$th entry of $\mB^{\left( 0 \right) }$ and the place of $i$ in $\mN^{\left( 0 \right) }$, resulting in new sets of basic and nonbasic indices $\mB^{\left( 1 \right) }, \mN^{\left( 1 \right) }$), we again obtain a system of the form (19), where every occurance of $^{\left( 0 \right) }$ is replaced with $^{\left( 1 \right) }$.

The above process of choosing which variable to \textit{leave} (i.e. $x_h$) and which to \textit{enter} (i.e. $x_s$)the set of basic variables is known as \textit{pivoting}. Also note that the new basic feasible solution $\vec{v}'$ that results from pivoting is again a vertex of $P$ by Lemma 5. In other words, what we described so far in this subsection is more-or-less an algebraic machinery of cheking optimality and choosing which neighboring vertex to move to, in case the current vertex is not optimal.

\subsection{Bland's Rule}

Observe that there could be more than one choice for the entering variable, and the same happens for the leaving variable. To deal with this, there are various \textit{pivot rules} used to systematically choose the variables from the indices. Unfortunately, under certain circumstances, known as \textit{degeneracy},\footnote{We call a linear program \textit{degenerate} if there exists a basic feasible solution that has at least one basic variable equal to 0.} bad choices of the pivot rule result in \textit{cycling} \cite{UULP}. Although it has known that cycling happens very rarely in practice \cite{UULP}, we nevertheless have certain pivot rules that guarantees to terminate without cycling, albeit inefficient. One of them is called \textit{Bland's rule}.

In short, \textit{Bland's rule always chooses smallest available index}. That is, whenever there is a tie for $s$ (i.e. the index for the leaving variable), out of the candidates, Bland's rule chooses the smallest one. It does so for $h$ (i.e. the index for the entering variable). More detailed explanation can be found in Bland's original paper \cite{NFPR}.

For us, it is more interesting to figure out how Bland's rule relates to Schrijver's algorithm. So suppose that we are given the inputs
\begin{equation*}
    \vec{a}_1,\ldots,\vec{a}_n,\vec{b}\in\F^m
\end{equation*}
for the algorithm, where we also know that $n,m\in\N$ and $\spn\left\lbrace \vec{a}_i \right\rbrace ^{n}_{i=1}=\F^m$. The step that we choose linearly independent $\vec{a}_{i_1},\ldots,\vec{a}_{i_m}$ that spans the whole space before the main iteration corresponds to choosing the initial basic variables, and the index set $\left\lbrace i_1,\ldots,i_m \right\rbrace$ corresponds to $\mB^{\left( 0 \right) }$ that we used in the description of the simplex method.

Then writing $\vec{b}$ as a linear combination of $\vec{a}_{i_1},\ldots,\vec{a}_{i_m}$ is equivalent to row-reducing the matrix $\begin{bmatrix}A_{\mB^{\left( 0 \right) }}&A_{\mN^{\left( 0 \right) }}|\vec{b}\end{bmatrix}$, and the coefficients $\lambda_{i_1},\ldots,\lambda_{i_m}$ are read-off from the rightmost column after the row-reduction. That is,
\begin{equation*}
    \begin{bmatrix} \lambda_{i_1}\\\vdots\\\lambda_{i_m} \end{bmatrix} = A_{\mB^{\left( 0 \right) }}^{-1} \vec{b}.
\end{equation*}
Note that $A=\begin{bmatrix} \vec{a}_1&\cdots&\vec{a}_n \end{bmatrix}$.

After this is where things get interesting. Bland's rule operates in phase II: it tells us how to move from a basic feasible solution to a better one (or one at least as good as the current) without cycling. But we are not there yet; we are struggling to figure out if the system $A\vec{x}=\vec{b}$ with $\vec{x}\geq\vec{0}$ is feasible.

Fortunately, we can use the following trick: instead of moving from a basic feasible solution to another basic feasible solution, we move from a basic solution to another basic solution.

In the description of phase II, note that we chose the entering variable first and then the leaving variable. However, for Schrijver's algorithm, note that we choose the leaving variable first. The reason is clear when we consider $A_{\mB^{\left( 0 \right) }}^{-1} \vec{b}$: we want this vector to have nonnegative entries only. That is, we desire to have an index set, say $\mB^{\left( p \right) }$, at some $p$th iteration such that $A_{\mB^{\left( 0 \right) }}^{-1} \vec{b}$ to be nonnegative. For this reason, the appropirate basic variables to be removed are the ones associated the rows of the row reduced matrix with negative entries at the end. These are exactly the candidates that Schrijver's algorithm has for $h$ (i.e. the leaving index), and it chooses the smallest index as Bland's rule does.

After choosing $h$, Schrijver's algorithm also chooses a vector $\vec{c}$ perpendicular to the linear hyperplane spanned by $\left\lbrace \vec{a}_{i_j} \right\rbrace ^n_{j=1,i_j\neq h}$ and lies at the same side with $\vec{a}_h$.\footnote{Any $n-1$ dimensional linear hyperplane in $\F^n$ separates the space into two disjoint sides.} Observe that, by defintion, $\vec{c}^{T} A$ is the $h$th row of $A_{\mB^{\left( 0 \right) }}^{-1} A=\begin{bmatrix} I & A_{\mB^{\left( 0 \right) }}^{-1} A_{\mN^{\left( 0 \right) }} \end{bmatrix}$ up to multiplying a positive scalar.\footnote{Equivalently, $\vec{c}^{T} $ is the $h$th row of $A_{\mB^{\left( 0 \right) }}^{-1} $ up to a constant.} This means the indices $j$'s such that $\vec{c}^{T} \vec{a}_j<0$ are the indices such that the $j$th entry of the $h$th row of row-reduced matrix is negative. Since the $h$th entry of $A_{\mB^{\left( 0 \right) }}^{-1} \vec{b}$ is also negative, it follows that we have to choose one of $j$'s in order to replace $x_h$ by $x_j$. So we choose the smallest $j$ as Bland's algorithm does, which we denoted as $s$, and replace $x_h$ by $x_s$.

Although what we are doing here is slightly different from the actual Bland's rule (which operates at phase II as remarked), proof presented by Schrijver, as shown in the previous section, shows that the analogous approach works.

\section{Further Topics}

\subsection{Consequences of the Fundamental Theorem}

A significant part of the fundamental theorem of linear inequalities is that it serves as a more-or-less \textit{fundamental lemma} to many theoretical results in linear programming. For instance, the famous \textit{Farkas' lemma} is a direct consequence of the theorem \cite{TLIP}.

\begin{theorem}[Farkas' Lemma]
    Let $A\in M_{m\times n}(\F) $ and let $\vec{b}\in\F^m$. Then there exists $\vec{x}\in\F^n$ with $A\vec{x}=\vec{b}, \vec{x}\geq\vec{0}$ if and only if $\vec{y}^{T} \vec{b}\geq 0$ for each $\vec{y}\in\F^m$ with $\vec{y}^{T} A\geq \vec{0}$.
\end{theorem}
\begin{proof}
    For the forward direction, suppose that there exists $\vec{x}\in\F^n$ with $A\vec{x}=\vec{b},\vec{x}\geq\vec{0}$. Then given any $\vec{y}\in\F^m$ such that $\vec{y}^{T} A\geq \vec{0}$, we have
    \begin{equation*}
        \vec{y}^{T} \vec{b} = \underbrace{\vec{y}^{T} A}_{\geq\vec{0}^{T} }\underbrace{\vec{x}}_{\geq\vec{0}} \geq 0.
    \end{equation*}
    Conversely, suppose that $A\vec{x}=\vec{b}, \vec{x}\geq\vec{0}$ is infeasible. Let $\vec{a}_1,\ldots,\vec{a}_n\in\F^m$ be the columns of $A$. Then $\vec{b}\notin\cone\left\lbrace \vec{a}_{j} \right\rbrace^{n}_{j=1}$ by the fundamental theorem of linear inequalities, and the theorem provides a certificate $\vec{c}$. This $\vec{c}$ is exactly what we want for $\vec{y}$.
\end{proof}

We also present other important consequences of the theorem without proofs. They are all from \textit{Schrijver's Theory of Linear and Integer Programming} \cite{TLIP}.

\begin{theorem}[Minkowski-Weyl Theorem]
    A convex cone in an Euclidean space is polyhedral if and only if finitely generated.
\end{theorem}

\begin{cor}[Decomposition Theorem]
    $P\subseteq\R^n$ is a polyhedron if and only if $P=Q+C$ for some polytope $Q\subseteq\R^n$ and polyhedral cone $C\subseteq\R^n$.
\end{cor}	

\begin{theorem}[Strong Duality Theorem]
    Let $A\in M_{m\times n}(\F), \vec{b}\in\F^m, \vec{c}\in\F^n$. Then
    \begin{equation*}
        \max\left\lbrace \vec{c}^{T} \vec{x}:A\vec{x}\leq\vec{b} \right\rbrace = \min\left\lbrace \vec{y}^{T} \vec{b}:\vec{y}\geq\vec{0},\vec{y}^{T} A=\vec{c}^{T}  \right\rbrace 
    \end{equation*}
    provided that the sets are nonempty.
\end{theorem}

\subsection{Phase I: More Efficient Feasibility Check}

As we noted in the previous section, cycling almost never happens in practice. For this reason, Bland's rule is not popular in practice \cite{UULP}, and we use different algorithms to check the feasibility of an LP problem. We demonstrate the one that is introduced by Dantzig \cite{LPE}. Suppose that we are given an LP problem
\begin{equation}
    \begin{aligned}
        \text{minimize } & \vec{c}^{T}\vec{x} && \\
        \text{subject to } & A\vec{x}=\vec{b} && \\  
                           & \vec{x}\geq\vec{0}
    \end{aligned},
\end{equation}
where $\vec{c}\in\F^n, \vec{b}\in\F^m$. We also assume that $\vec{b}\geq\vec{0}$, since we may multiply -1 to both sides of any equality. We then introduce \textit{artificial variables} (or \textit{error variables}) $x_{n+1},\ldots,x_{n+m}$. That is, we consider an LP problem
\begin{equation}
    \begin{aligned}
        \text{minimize } & \sum^{m}_{j=1} x_{n+j} && \\
        \text{subject to } &  \begin{bmatrix} A&I \end{bmatrix} \vec{z} && \\
                           & \vec{z}\geq\vec{0}
    \end{aligned},
\end{equation}
where $\vec{z}=\left( \vec{x},x_{n+1},\ldots,x_{n+m} \right) $. Note that (21) is feasible doubtlessly, by setting $\vec{x}=0$ and $x_{n+j}=b_j$ for all $j\in\left\lbrace 1,\ldots,m \right\rbrace$, where $b_j$ is the $j$th entry of $\vec{b}$. Then, instead of using Bland's rule, we use Dantzig's rule (see \cite{LPE}) for pivoting. 

After obtaining an optimal solution for (21), if the optimal objective value is 0, then the optimal solution we have is nothing more than a basic feasible solution for (20). Hence we proceed to phase II. Otherwise, if the optimal objective value is strictly greater than 0, then we know (20) is not feasible.

\subsection{Implementing Schrijver's Algorithm}

Implementing Schrijver's algorithm is as straightforward as its description. An implementation can be found at \texttt{https://github.com/snochi/schrijver}.






























\clearpage

\bibliography{report}
\bibliographystyle{icml2020}

\end{document}
